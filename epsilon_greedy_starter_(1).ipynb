{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MonikaLamba/Reinforcement-Learning/blob/main/epsilon_greedy_starter_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "owUKe-LUo4Gx"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_TRIALS = 10000\n",
        "EPS = 0.1\n",
        "BANDIT_PROBABILITIES = [0.2, 0.5, 0.75]"
      ],
      "metadata": {
        "id": "1zuo8TcWpSiY"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Bandit:\n",
        "  def _init_(self,p):\n",
        "    # pretending dont know p value\n",
        "    #p: the win rate\n",
        "    # THERE ARE THREE INSTANCE VARIABLE WHERE P REPRESNT TRUE WIN RATE\n",
        "    self.p = p\n",
        "    self.p_estimate= 0. #TODO\n",
        "    self.N = 0. # TODO NUM OF SAMPLES COLLECTED SO FAR\n",
        "    # N represent no. samples\n",
        "    #TODO -- INITIALIZE THEM CORRECTLY\n",
        "\n",
        "    #PULL FUNCTION HAS BEEN COMPLETED. RETURN A ONE WITH PROBABLITY P\n",
        "  def pull(self):\n",
        "    #draw a 1 with probabilty p\n",
        "    return np.random.random() < self.p\n",
        "# TRUE IS EQUIVALENT TO 1 IN PYTHON AND RETURNING A BOOLEAN\n",
        "  def update(self,x):\n",
        "    self.N += 1. #TODO HERE X VALUE CAN BE ZERO OR 1\n",
        "    self.p_estimate = ((self.N -1)* self.p_estimate + x)/self.N # TODO\n",
        "    # x = SAMPLE VALUE AND USES TO UPDATE THE P ESTIMATE\n",
        "\n",
        "#experiment function which is going to run Epilson Greedy algorithm\n",
        "#initialize list of bandits objects with Win rates equal to the bandid probaility constant\n",
        "  def experiment():\n",
        "    bandits = [Bandit(p) for p in BANDIT_PROBABILITIES]\n",
        "\n",
        "    rewards = np.zeros(NUM_TRIALS)\n",
        "    num_times_explored = 0\n",
        "    num_times_exploited = 0\n",
        "    num_optimal = 0\n",
        "    optimal_j = np.argmax([b.p for b in bandits])\n",
        "    print(\"optimal j:\", optimal_j)\n",
        "\n",
        "    for i in range(NUM_TRIALS):\n",
        "\n",
        "    # use epilson-greedy to select the next bandit\n",
        "     if np.random.random() < EPS:\n",
        "      num_times_explored += 1\n",
        "      j = np.random.randint(len(bandits)) # TODO\n",
        "\n",
        "      #j will be an integer from 0 to number of bandit - 1 and used to index the list of bandits\n",
        "     else:\n",
        "      num_times_exploited += 1\n",
        "      j= np.argmax([b.p_estimate for b in bandits]) #TODO\n",
        "\n",
        "     if j ==optimal_j:\n",
        "      optimal_j += 1\n",
        "\n",
        "      # pull the arm for the bandit with the largest sample\n",
        "      x = bandits[j].pull()\n",
        "\n",
        "      #update rewards log\n",
        "\n",
        "      rewards[i] = x\n",
        "\n",
        "      #update the distribition for the bandit whose arm we just pulled\n",
        "      # update bandit mean by passing the function update when loop is over\n",
        "      bandits[j].update(x)\n",
        "\n",
        "# print mean estimates for each bandit\n",
        "    for b in bandits:\n",
        "      print(\"mean estimate:\", b.p_estimate)\n",
        "\n",
        "      #print total reward\n",
        "\n",
        "    print(\"total reward earned:\", rewards.sum())\n",
        "    print(\"overall win rate:\", rewards.sum()/NUM_TRIALS)\n",
        "    print(\"num_times_explored:\", num_times_explored)\n",
        "    print(\"num_times_exploited:\", num_times_exploited)\n",
        "    print(\"num times selected optimal bandit:\", num_optimal)\n",
        "\n",
        "\n",
        "     # plot the results\n",
        "    cumulative_rewards = np.cumsum(rewards)\n",
        "    win_rates = cumulative_rewards/ np.arange(NUM_TRIALS)+1\n",
        "    plt.plot(win_rates)\n",
        "    plt.plot(np.ones(NUM_TRIALS)*np.max(BANDIT_PROBABILITIES))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == \"_main_\":\n",
        "  experiment()\n"
      ],
      "metadata": {
        "id": "kXlimkgFpcYe"
      },
      "execution_count": 51,
      "outputs": []
    }
  ]
}